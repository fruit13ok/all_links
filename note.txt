// to begin this project
mkdir all_links
cd all_links
code .
npm init -y
npm install express puppeteer body-parser node-fetch cors jspdf jspdf-autotable googleapis@39 abort-controller --save
npm install mongoose dotenv --save
npm install nodemon --save-dev

// in package.json
    "start-dev": "nodemon server"

// create basic back end and front end directories and files
mkdir server
touch server/index.js
mkdir public
touch public/app.js public/index.html

// server/index.js
// After installed all packages, create ROOT and POST routes,
// ROOT route just be there, 
// POST route takes form input request and response back its input for now,
// should be able to test with Postman.

// public/app.js
// Access index.html form, on form submit, fetch a post request to backend index.js,
// get JSON object response back as array of form input for now,
// append response as HTML list below.

// public/index.html
// Access app.js, build a form with 3 input fields, a button, and a empty div,
// result will later append to empty div.

// run the server
npm run start-dev

// Without front end is ok, this is a back end only project that produce JSON result,
// This front end got redirected the path is for testing, everything can be test with Postman.

// upload to github as first milestone
echo "# all_links" >> README.md
touch .gitignore
    // add 2 lines
    # dependencies
    /node_modules
    node_modules/
git init
git add .
git commit -m "first commit, basic server with form POST"
git remote add origin https://github.com/fruit13ok/all_links.git
git push -u origin master

// if .gitignore not working, having upload project too large error,
// delete node_modules folder, push again,
// if still not work, delete .git folder too, try again.
// Remember to npm i before next run, might need to click refresh in folder explorer.


//////////////////// implement scrape for all links //////////////////////

1).
format input url into standard url, 
<<< this might not suit all url, take care later >>>
EX:
https://www.google.com

2).
scrape for all links, select 'a' tag's 'href', map nodelist to array, 
<<< assume no login nor personalization on the given page >>>
EX:
await page.$$eval('a', as => as.map(a => a.href));

3).
[[[ This takes time, due to number of page fetch request ]]]
[[[ CAREFUL !!! many asynchronous, normal loop don't work ]]]
check url status by send fetch request to each urls, 
fetch function is asynchronous
EX:
checkUrl(url)
loop over each fetch function also need to be asynchronous, 
format the return result here
EX:
forLoop(resultArr)
they start after scraped those urls
EX:
scrape(targetPage)


//////////////////// implement JSON to PDF //////////////////////
I added JSON to PDF feature base on this example:
https://github.com/fruit13ok/try_jspdf

//////////////////// implement JSON to CSV //////////////////////
I added JSON to CSV feature

//////////////////////////////////////////////////////////////////
///////////// add MongoDB, MLAB, .env, POST, GET all /////////////
//////////////////////////////////////////////////////////////////

// insatll mongoose
npm install mongoose --save

// need mlab account
https://mlab.com/
fruit13ok
ly13OK

    // after login, create new DB, create a user can access this DB
    https://mlab.com/home?newLogin=1
    "Create new" -> "SANDBOX" continue -> "US East" continue 
    -> name it "scrapedb" continue -> "Submit order"
    -> click on "scrapedb" -> "Users" Add database user
    -> fruit13ok / ly13OK

// to connect
// MongoDB URI
mongodb://<dbuser>:<dbpassword>@ds015889.mlab.com:15889/scrapedb
//mongo shell
% mongo ds015889.mlab.com:15889/scrapedb -u <dbuser> -p <dbpassword>

// use mongoose in code
// in index.js
const mongoose = require('mongoose');
mongoose.connect('mongodb://fruit13ok:ly13OK@ds015889.mlab.com:15889/scrapedb',
    { useNewUrlParser: true, useUnifiedTopology: true }, 
    () => {console.log('connected to DB');}
);

// install dorenv
npm install dotenv --save

// create .env file
mkdir .env
    // edit .gitignore
    .env

// use dotenv in code to store key
// in .env
DB_CONNECTION=mongodb://fruit13ok:ly13OK@ds015889.mlab.com:15889/scrapedb
// in index.js
require('dotenv/config');
// to access env variables, use process.env.THE_VARIABLE_NAME
EX:
mongoose.connect(process.env.DB_CONNECTION,
    { useNewUrlParser: true, useUnifiedTopology: true }, 
    () => {console.log('connected to DB');}
);

// use mongoose to create models, 
// data will be a bit more than the result,
// it contain the given link, and scrape result with array of url and status,
// in DB it will have _id too
create models/Scrapelink.js
create Schema us subdocument way to create nested array of object,
there are 2 Schema work together,
so in DB there are 2 set of _id

// export only the outter model from Scrapelink.js, so index.js can erequire it
const Scrapelink = mongoose.model('Scrapelink', ScrapelinkSchema);
module.exports = Scrapelink;

// in index.js POST route, after finished scraping done, 
// use the Scrapelink model to pass in target page and result array,
// there will be smartly create collection if not exist, add document if exist,
const scrapelink = new Scrapelink({
    givenlink: targetPage,
    resultlinks: resultArray
});
scrapelink.save()
.then(data => {
    res.send(data.resultlinks);
})
.catch(err => {
    res.send("DB ERROR: ", err);
});

// in index.js create a GET route for http://localhost:8000/api
// get all documents from Scrapelink collection, response back JSON
app.get('/api', async function (req, res) {
const allSavedData = await Scrapelink.find();
res.send(allSavedData);

// the DB will add _id to each document, that mean each result saved to DB,
// also add _id to each url/status set,

// in html add "Show Saved Results" button to triger fetch all data from DB

// treat the whole data came back from DB as a JSON with extra _id column in first level of object, 
// and with in each result set in second level,

// !!! NOTE !!! before added DB, result after scrape no _id column, send to frontend,
// !!! NOTE !!! after added DB, after scrape, send to DB, now has _id column, send to frontend,

// [[ OPTIONAL ]] to display to screen and CVS download I removed both _id column,
// So it look similar like before adding DB, no need to change CSV
    // right after sent result to DB, I use data response back from DB, send to frontend
    // it is a "document" 
    // EX: 
    [{_id,url,status},{_id,url,status},...]
        // only 1 level deep, I remove _id by map()
        let json = await response.json();
        json = json.map(key => { 
            delete key._id; 
            return key; 
        });
    // when fetch all data from DB,
    // it is a "collection" that contain multiple "document"
    // EX:
    [
        {_id,givenlink,resultlinks:[{_id,url,status},{_id,url,status},...]},
        {_id,givenlink,resultlinks:[{_id,url,status},{_id,url,status},...]}
    ]
        // 2 levels deep, I remove _id by forEach() loop through collection, map() each document
        const json = await response.json();
        json.forEach(e => {
            e.resultlinks.map(k => {
                delete k._id;
                return k;
            });
        });

// display on screen mostly the same

////////////////////////////////////////////////////////////////////
// side tack a little bit, refactor PDF and CSV code to functions //
////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////
///////////////////////// google sheet api /////////////////////////
//                      see sheets-api-project                    //
////////////////////////////////////////////////////////////////////

// the credential and token were made from testing project sheets-api-project
// I just copied them to server folder, change the path in code to match




