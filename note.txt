// to begin this project
mkdir all_links
cd all_links
code .
npm init -y
npm install express puppeteer body-parser node-fetch cors --save
npm install nodemon --save-dev

// in package.json
    "start-dev": "nodemon server"

// create basic back end and front end directories and files
mkdir server
touch server/index.js
mkdir public
touch public/app.js public/index.html

// server/index.js
// After installed all packages, create ROOT and POST routes,
// ROOT route just be there, 
// POST route takes form input request and response back its input for now,
// should be able to test with Postman.

// public/app.js
// Access index.html form, on form submit, fetch a post request to backend index.js,
// get JSON object response back as array of form input for now,
// append response as HTML list below.

// public/index.html
// Access app.js, build a form with 3 input fields, a button, and a empty div,
// result will later append to empty div.

// run the server
npm run start-dev

// Without front end is ok, this is a back end only project that produce JSON result,
// This front end got redirected the path is for testing, everything can be test with Postman.

// upload to github as first milestone
echo "# all_links" >> README.md
touch .gitignore
    // add 2 lines
    # dependencies
    /node_modules
    node_modules/
git init
git add .
git commit -m "first commit, basic server with form POST"
git remote add origin https://github.com/fruit13ok/all_links.git
git push -u origin master

// if .gitignore not working, having upload project too large error,
// delete node_modules folder, push again,
// if still not work, delete .git folder too, try again.
// Remember to npm i before next run, might need to click refresh in folder explorer.


//////////////////// implement scrape for all links //////////////////////

1).
format input url into standard url, 
<<< this might not suit all url, take care later >>>
EX:
https://www.google.com

2).
scrape for all links, select 'a' tag's 'href', map nodelist to array, 
<<< assume no login nor personalization on the given page >>>
EX:
await page.$$eval('a', as => as.map(a => a.href));

3).
[[[ This takes time, due to number of page fetch request ]]]
[[[ CAREFUL !!! many asynchronous, normal loop don't work ]]]
check url status by send fetch request to each urls, 
fetch function is asynchronous
EX:
checkUrl(url)
loop over each fetch function also need to be asynchronous, 
format the return result here
EX:
forLoop(resultArr)
they start after scraped those urls
EX:
scrape(targetPage)

